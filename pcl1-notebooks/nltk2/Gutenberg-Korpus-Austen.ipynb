{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Representation of pure text corpora\n",
    "Learning goals:\n",
    " - Understand the functionality of the Gutenberg corpus reader object for English raw texts\n",
    " - Understand how raw text corpora can be  represented on different levels: character string, token list, sentence list, paragraph list\n",
    " - Understand that Gutenberg is just an instance of the PlaintextCorpusReader class\n",
    " - Understand how this PlaintextCorpusReader can be adapted to other languages than English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('/Users/siclemat/nltk_data/corpora/gutenberg')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Where are the text files stored?\n",
    "gutenberg.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PlaintextCorpusReader in module nltk.corpus.reader.plaintext object:\n",
      "\n",
      "class PlaintextCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      " |  PlaintextCorpusReader(root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL), sent_tokenizer=None, para_block_reader=<function read_blankline_block at 0x112217240>, encoding='utf8')\n",
      " |\n",
      " |  Reader for corpora that consist of plaintext documents.  Paragraphs\n",
      " |  are assumed to be split using blank lines.  Sentences and words can\n",
      " |  be tokenized using the default tokenizers, or by custom tokenizers\n",
      " |  specified as parameters to the constructor.\n",
      " |\n",
      " |  This corpus reader can be customized (e.g., to skip preface\n",
      " |  sections of specific document formats) by creating a subclass and\n",
      " |  overriding the ``CorpusView`` class variable.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      PlaintextCorpusReader\n",
      " |      nltk.corpus.reader.api.CorpusReader\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL), sent_tokenizer=None, para_block_reader=<function read_blankline_block at 0x112217240>, encoding='utf8')\n",
      " |      Construct a new plaintext corpus reader for a set of documents\n",
      " |      located at the given root directory.  Example usage:\n",
      " |\n",
      " |          >>> root = '/usr/local/share/nltk_data/corpora/webtext/'\n",
      " |          >>> reader = PlaintextCorpusReader(root, '.*\\.txt') # doctest: +SKIP\n",
      " |\n",
      " |      :param root: The root directory for this corpus.\n",
      " |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      " |      :param word_tokenizer: Tokenizer for breaking sentences or\n",
      " |          paragraphs into words.\n",
      " |      :param sent_tokenizer: Tokenizer for breaking paragraphs\n",
      " |          into words.\n",
      " |      :param para_block_reader: The block reader used to divide the\n",
      " |          corpus into paragraph blocks.\n",
      " |\n",
      " |  paras(self, fileids=None)\n",
      " |      :return: the given file(s) as a list of\n",
      " |          paragraphs, each encoded as a list of sentences, which are\n",
      " |          in turn encoded as lists of word strings.\n",
      " |      :rtype: list(list(list(str)))\n",
      " |\n",
      " |  sents(self, fileids=None)\n",
      " |      :return: the given file(s) as a list of\n",
      " |          sentences or utterances, each encoded as a list of word\n",
      " |          strings.\n",
      " |      :rtype: list(list(str))\n",
      " |\n",
      " |  words(self, fileids=None)\n",
      " |      :return: the given file(s) as a list of words\n",
      " |          and punctuation symbols.\n",
      " |      :rtype: list(str)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      " |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      " |      it can be accessed by index, iterated over, etc.  However, the\n",
      " |      tokens are only constructed as-needed -- the entire corpus is\n",
      " |      never stored in memory at once.\n",
      " |\n",
      " |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      " |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      " |      and a block reader.  A \"block reader\" is a function that reads\n",
      " |      zero or more tokens from a stream, and returns them as a list.  A\n",
      " |      very simple example of a block reader is:\n",
      " |\n",
      " |          >>> def simple_block_reader(stream):\n",
      " |          ...     return stream.readline().split()\n",
      " |\n",
      " |      This simple block reader reads a single line at a time, and\n",
      " |      returns a single token (consisting of a string) for each\n",
      " |      whitespace-separated substring on the line.\n",
      " |\n",
      " |      When deciding how to define the block reader for a given\n",
      " |      corpus, careful consideration should be given to the size of\n",
      " |      blocks handled by the block reader.  Smaller block sizes will\n",
      " |      increase the memory requirements of the corpus view's internal\n",
      " |      data structures (by 2 integers per block).  On the other hand,\n",
      " |      larger block sizes may decrease performance for random access to\n",
      " |      the corpus.  (But note that larger block sizes will *not*\n",
      " |      decrease performance for iteration.)\n",
      " |\n",
      " |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      " |      index to file position, with one entry per block.  When a token\n",
      " |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      " |      it as follows:\n",
      " |\n",
      " |        1. First, it searches the toknum/filepos mapping for the token\n",
      " |           index closest to (but less than or equal to) *i*.\n",
      " |\n",
      " |        2. Then, starting at the file position corresponding to that\n",
      " |           index, it reads one block at a time using the block reader\n",
      " |           until it reaches the requested token.\n",
      " |\n",
      " |      The toknum/filepos mapping is created lazily: it is initially\n",
      " |      empty, but every time a new block is read, the block's\n",
      " |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      " |      map has one entry per block.)\n",
      " |\n",
      " |      In order to increase efficiency for random access patterns that\n",
      " |      have high degrees of locality, the corpus view may cache one or\n",
      " |      more blocks.\n",
      " |\n",
      " |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      " |          object for its underlying corpus file.  This file should be\n",
      " |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      " |          but if you wish to close it manually, use the ``close()``\n",
      " |          method.  If you access a ``CorpusView``'s items after it has been\n",
      " |          closed, the file object will be automatically re-opened.\n",
      " |\n",
      " |      :warning: If the contents of the file are modified during the\n",
      " |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      " |          is undefined.\n",
      " |\n",
      " |      :warning: If a unicode encoding is specified when constructing a\n",
      " |          ``CorpusView``, then the block reader may only call\n",
      " |          ``stream.seek()`` with offsets that have been returned by\n",
      " |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      " |          relative offsets, or with offsets based on string lengths, may\n",
      " |          lead to incorrect behavior.\n",
      " |\n",
      " |      :ivar _block_reader: The function used to read\n",
      " |          a single block from the underlying file stream.\n",
      " |      :ivar _toknum: A list containing the token index of each block\n",
      " |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      " |          token index of the first token in block ``i``.  Together\n",
      " |          with ``_filepos``, this forms a partial mapping between token\n",
      " |          indices and file positions.\n",
      " |      :ivar _filepos: A list containing the file position of each block\n",
      " |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      " |          file position of the first character in block ``i``.  Together\n",
      " |          with ``_toknum``, this forms a partial mapping between token\n",
      " |          indices and file positions.\n",
      " |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      " |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      " |          or None, if the number of tokens is not yet known.\n",
      " |      :ivar _eofpos: The character position of the last character in the\n",
      " |          file.  This is calculated when the corpus view is initialized,\n",
      " |          and is used to decide when the end of file has been reached.\n",
      " |      :ivar _cache: A cache of the most recently read block.  It\n",
      " |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      " |         start_toknum is the token index of the first token in the block;\n",
      " |         end_toknum is the token index of the first token not in the\n",
      " |         block; and tokens is a list of the tokens in the block.\n",
      " |\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  abspath(self, fileid)\n",
      " |      Return the absolute path for the given file.\n",
      " |\n",
      " |      :type fileid: str\n",
      " |      :param fileid: The file identifier for the file whose path\n",
      " |          should be returned.\n",
      " |      :rtype: PathPointer\n",
      " |\n",
      " |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      " |      Return a list of the absolute paths for all fileids in this corpus;\n",
      " |      or for the given list of fileids, if specified.\n",
      " |\n",
      " |      :type fileids: None or str or list\n",
      " |      :param fileids: Specifies the set of fileids for which paths should\n",
      " |          be returned.  Can be None, for all fileids; a list of\n",
      " |          file identifiers, for a specified set of fileids; or a single\n",
      " |          file identifier, for a single file.  Note that the return\n",
      " |          value is always a list of paths, even if ``fileids`` is a\n",
      " |          single file identifier.\n",
      " |\n",
      " |      :param include_encoding: If true, then return a list of\n",
      " |          ``(path_pointer, encoding)`` tuples.\n",
      " |\n",
      " |      :rtype: list(PathPointer)\n",
      " |\n",
      " |  citation(self)\n",
      " |      Return the contents of the corpus citation.bib file, if it exists.\n",
      " |\n",
      " |  encoding(self, file)\n",
      " |      Return the unicode encoding for the given corpus file, if known.\n",
      " |      If the encoding is unknown, or if the given file should be\n",
      " |      processed using byte strings (str), then return None.\n",
      " |\n",
      " |  ensure_loaded(self)\n",
      " |      Load this corpus (if it has not already been loaded).  This is\n",
      " |      used by LazyCorpusLoader as a simple method that can be used to\n",
      " |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      " |      do help(some_corpus).\n",
      " |\n",
      " |  fileids(self)\n",
      " |      Return a list of file identifiers for the fileids that make up\n",
      " |      this corpus.\n",
      " |\n",
      " |  license(self)\n",
      " |      Return the contents of the corpus LICENSE file, if it exists.\n",
      " |\n",
      " |  open(self, file)\n",
      " |      Return an open stream that can be used to read the given file.\n",
      " |      If the file's encoding is not None, then the stream will\n",
      " |      automatically decode the file's contents into unicode.\n",
      " |\n",
      " |      :param file: The file identifier of the file to read.\n",
      " |\n",
      " |  raw(self, fileids=None)\n",
      " |      :param fileids: A list specifying the fileids that should be used.\n",
      " |      :return: the given file(s) as a single string.\n",
      " |      :rtype: str\n",
      " |\n",
      " |  readme(self)\n",
      " |      Return the contents of the corpus README file, if it exists.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |\n",
      " |  root\n",
      " |      The directory where this corpus is stored.\n",
      " |\n",
      " |      :type: PathPointer\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gutenberg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text as a long string: method raw()\n",
    "\n",
    "- Text = sequence of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"--But, in spite of these deficiencies, the wishes,\\nthe hopes, the confidence, the predictions of the small band\\nof true friends who witnessed the ceremony, were fully answered\\nin the perfect happiness of the union.\\n\\n\\nFINIS\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_chars = gutenberg.raw(\"austen-emma.txt\")\n",
    "emma_chars[-224:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text as sequence of words: method words()\n",
    " - Text = sequence of words\n",
    " - Word = sequence of characters = string\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'and',\n",
       " 'rich',\n",
       " ',',\n",
       " 'with',\n",
       " 'a',\n",
       " 'comfortable',\n",
       " 'home',\n",
       " 'and',\n",
       " 'happy',\n",
       " 'disposition',\n",
       " ',',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'unite',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'best',\n",
       " 'blessings',\n",
       " 'of',\n",
       " 'existence',\n",
       " ';']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename=\"austen-emma.txt\"\n",
    "emma_words = gutenberg.words(filename)\n",
    "emma_words[11:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text as a sequence of sentences: Method sents()\n",
    " - Text = sequence of sentences\n",
    " - sentence = sequence of words\n",
    " - Word = sequence of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['--',\n",
       "  'But',\n",
       "  ',',\n",
       "  'in',\n",
       "  'spite',\n",
       "  'of',\n",
       "  'these',\n",
       "  'deficiencies',\n",
       "  ',',\n",
       "  'the',\n",
       "  'wishes',\n",
       "  ',',\n",
       "  'the',\n",
       "  'hopes',\n",
       "  ',',\n",
       "  'the',\n",
       "  'confidence',\n",
       "  ',',\n",
       "  'the',\n",
       "  'predictions',\n",
       "  'of',\n",
       "  'the',\n",
       "  'small',\n",
       "  'band',\n",
       "  'of',\n",
       "  'true',\n",
       "  'friends',\n",
       "  'who',\n",
       "  'witnessed',\n",
       "  'the',\n",
       "  'ceremony',\n",
       "  ',',\n",
       "  'were',\n",
       "  'fully',\n",
       "  'answered',\n",
       "  'in',\n",
       "  'the',\n",
       "  'perfect',\n",
       "  'happiness',\n",
       "  'of',\n",
       "  'the',\n",
       "  'union',\n",
       "  '.'],\n",
       " ['FINIS']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_sents = gutenberg.sents(filename)\n",
    "\n",
    "# Last 2 sentences\n",
    "emma_sents[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document as a sequence of paragraphs: method paras()\n",
    " - corpus = sequence of paragraphs\n",
    " - paragraph = sequence of sentences\n",
    " - sentence = sequence of words\n",
    " - word = sequence of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']],\n",
       " [['VOLUME', 'I']],\n",
       " [['CHAPTER', 'I']],\n",
       " [['Emma',\n",
       "   'Woodhouse',\n",
       "   ',',\n",
       "   'handsome',\n",
       "   ',',\n",
       "   'clever',\n",
       "   ',',\n",
       "   'and',\n",
       "   'rich',\n",
       "   ',',\n",
       "   'with',\n",
       "   'a',\n",
       "   'comfortable',\n",
       "   'home',\n",
       "   'and',\n",
       "   'happy',\n",
       "   'disposition',\n",
       "   ',',\n",
       "   'seemed',\n",
       "   'to',\n",
       "   'unite',\n",
       "   'some',\n",
       "   'of',\n",
       "   'the',\n",
       "   'best',\n",
       "   'blessings',\n",
       "   'of',\n",
       "   'existence',\n",
       "   ';',\n",
       "   'and',\n",
       "   'had',\n",
       "   'lived',\n",
       "   'nearly',\n",
       "   'twenty',\n",
       "   '-',\n",
       "   'one',\n",
       "   'years',\n",
       "   'in',\n",
       "   'the',\n",
       "   'world',\n",
       "   'with',\n",
       "   'very',\n",
       "   'little',\n",
       "   'to',\n",
       "   'distress',\n",
       "   'or',\n",
       "   'vex',\n",
       "   'her',\n",
       "   '.']]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_paras = gutenberg.paras(filename)\n",
    "emma_paras[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Corpus linguistic questions\n",
    "How many paragraphs does \"Emma\" have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2371"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emma_paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How many sentences does \"Emma\" have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7752"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emma_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How many sentences does a paragraph have on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2695065373260226"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emma_sents)/len(emma_paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How can we format that nicely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Average # of sentence per paragraph: 3.27'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg = len(emma_sents)/len(emma_paras)\n",
    "f\"Average # of sentence per paragraph: {avg:.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Read your own text corpora\n",
    "Explicit correct decoding of the text file can be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Die', 'Allgemeine', 'Erklärung', 'der', 'Menschenrechte', 'Resolution', '217', 'A', '(', 'III', ')', 'vom', '10', '.', '12', '.', '1948'], ['Präambel', 'Da', 'die', 'Anerkennung', 'der', 'angeborenen', 'Würde', 'und', 'der', 'gleichen', 'und', 'unveräußerlichen', 'Rechte', 'aller', 'Mitglieder', 'der', 'Gemeinschaft', 'der', 'Menschen', 'die', 'Grundlage', 'von', 'Freiheit', ',', 'Gerechtigkeit', 'und', 'Frieden', 'in', 'der', 'Welt', 'bildet', ',', 'da', 'die', 'Nichtanerkennung', 'und', 'Verachtung', 'der', 'Menschenrechte', 'zu', 'Akten', 'der', 'Barbarei', 'geführt', 'haben', ',', 'die', 'das', 'Gewissen', 'der', 'Menschheit', 'mit', 'Empörung', 'erfüllen', ',', 'und', 'da', 'verkündet', 'worden', 'ist', ',', 'daß', 'einer', 'Welt', ',', 'in', 'der', 'die', 'Menschen', 'Rede', '-', 'und', 'Glaubensfreiheit', 'und', 'Freiheit', 'von', 'Furcht', 'und', 'Not', 'genießen', ',', 'das', 'höchste', 'Streben', 'des', 'Menschen', 'gilt', ',', 'da', 'es', 'notwendig', 'ist', ',', 'die', 'Menschenrechte', 'durch', 'die', 'Herrschaft', 'des', 'Rechtes', 'zu', 'schützen', ',', 'damit', 'der', 'Mensch', 'nicht', 'gezwungen', 'wird', ',', 'als', 'letztes', 'Mittel', 'zum', 'Aufstand', 'gegen', 'Tyrannei', 'und', 'Unterdrückung', 'zu', 'greifen', ',', 'da', 'es', 'notwendig', 'ist', ',', 'die', 'Entwicklung', 'freundschaftlicher', 'Beziehungen', 'zwischen', 'den', 'Nationen', 'zu', 'fördern', ',', 'da', 'die', 'Völker', 'der', 'Vereinten', 'Nationen', 'in', 'der', 'Charta', 'ihren', 'Glauben', 'an', 'die', 'grundlegenden', 'Menschenrechte', ',', 'an', 'die', 'Würde', 'und', 'den', 'Wert', 'der', 'menschlichen', 'Person', 'und', 'an', 'die', 'Gleichberechtigung', 'von', 'Mann', 'und', 'Frau', 'erneut', 'bekräftigt', 'und', 'beschlossen', 'haben', ',', 'den', 'sozialen', 'Fortschritt', 'und', 'bessere', 'Lebensbedingungen', 'in', 'größerer', 'Freiheit', 'zu', 'fördern', ',', 'da', 'die', 'Mitgliedstaaten', 'sich', 'verpflichtet', 'haben', ',', 'in', 'Zusammenarbeit', 'mit', 'den', 'Vereinten', 'Nationen', 'auf', 'die', 'allgemeine', 'Achtung', 'und', 'Einhaltung', 'der', 'Menschenrechte', 'und', 'Grundfreiheiten', 'hinzuwirken', ',', 'da', 'ein', 'gemeinsames', 'Verständnis', 'dieser', 'Rechte', 'und', 'Freiheiten', 'von', 'größter', 'Wichtigkeit', 'für', 'die', 'volle', 'Erfüllung', 'dieser', 'Verpflichtung', 'ist', ',', 'verkündet', 'die', 'Generalversammlung', 'diese', 'Allgemeine', 'Erklärung', 'der', 'Menschenrechte', 'als', 'das', 'von', 'allen', 'Völkern', 'und', 'Nationen', 'zu', 'erreichende', 'gemeinsame', 'Ideal', ',', 'damit', 'jeder', 'einzelne', 'und', 'alle', 'Organe', 'der', 'Gesellschaft', 'sich', 'diese', 'Erklärung', 'stets', 'gegenwärtig', 'halten', 'und', 'sich', 'bemühen', ',', 'durch', 'Unterricht', 'und', 'Erziehung', 'die', 'Achtung', 'vor', 'diesen', 'Rechten', 'und', 'Freiheiten', 'zu', 'fördern', 'und', 'durch', 'fortschreitende', 'nationale', 'und', 'internationale', 'Maßnahmen', 'ihre', 'allgemeine', 'und', 'tatsächliche', 'Anerkennung', 'und', 'Einhaltung', 'durch', 'die', 'Bevölkerung', 'der', 'Mitgliedstaaten', 'selbst', 'wie', 'auch', 'durch', 'die', 'Bevölkerung', 'der', 'ihrer', 'Hoheitsgewalt', 'unterstehenden', 'Gebiete', 'zu', 'gewährleisten', '.'], ['Artikel', '1', 'Alle', 'Menschen', 'sind', 'frei', 'und', 'gleich', 'an', 'Würde', 'und', 'Rechten', 'geboren', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "root = '/Users/siclemat/nltk_data/corpora/udhr2/'\n",
    "file_pattern = r'.+\\.txt'\n",
    "my_humanrights = PlaintextCorpusReader(root,\n",
    "                    file_pattern,\n",
    "                    encoding='utf-8')\n",
    "\n",
    "print(my_humanrights.sents('deu_1901.txt')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How many declarations have been collected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "007.txt\t\t   ell_polytonic.txt  lia.txt\t\t rmy.txt\r\n",
      "008.txt\t\t   emk.txt\t      lin.txt\t\t roh.txt\r\n",
      "009.txt\t\t   eml.txt\t      lin_tones.txt\t ron.txt\r\n",
      "010.txt\t\t   eng.txt\t      lit.txt\t\t ron_1953.txt\r\n",
      "011.txt\t\t   epo.txt\t      lnc.txt\t\t ron_1993.txt\r\n",
      "abk.txt\t\t   est.txt\t      lns.txt\t\t run.txt\r\n",
      "ace.txt\t\t   eus.txt\t      lot.txt\t\t rus.txt\r\n",
      "acu.txt\t\t   eve.txt\t      loz.txt\t\t sag.txt\r\n",
      "acu_1.txt\t   evn.txt\t      ltz.txt\t\t sah.txt\r\n",
      "ada.txt\t\t   ewe.txt\t      lua.txt\t\t san.txt\r\n",
      "afr.txt\t\t   fao.txt\t      lue.txt\t\t sco.txt\r\n",
      "agr.txt\t\t   fij.txt\t      lug.txt\t\t sey.txt\r\n",
      "aii.txt\t\t   fin.txt\t      lun.txt\t\t shk.txt\r\n",
      "ajg.txt\t\t   flm.txt\t      lus.txt\t\t shp.txt\r\n",
      "aka_akuapem.txt    fon.txt\t      mad.txt\t\t skr.txt\r\n",
      "aka_asante.txt\t   fra.txt\t      mag.txt\t\t slk.txt\r\n",
      "aka_fante.txt\t   fri.txt\t      mah.txt\t\t slv.txt\r\n",
      "als.txt\t\t   fuc.txt\t      mai.txt\t\t sme.txt\r\n",
      "alt.txt\t\t   fur.txt\t      mal.txt\t\t smo.txt\r\n",
      "amc.txt\t\t   gaa.txt\t      mam.txt\t\t sna.txt\r\n",
      "ame.txt\t\t   gag.txt\t      mar.txt\t\t snk.txt\r\n",
      "amh.txt\t\t   gax.txt\t      maz.txt\t\t snn.txt\r\n",
      "amr.txt\t\t   gjn.txt\t      mcd.txt\t\t som.txt\r\n",
      "arb.txt\t\t   gkp.txt\t      mcf.txt\t\t sot.txt\r\n",
      "arl.txt\t\t   gla.txt\t      men.txt\t\t spa.txt\r\n",
      "arn.txt\t\t   gle.txt\t      mic.txt\t\t src.txt\r\n",
      "ast.txt\t\t   glg.txt\t      min.txt\t\t srp_cyrl.txt\r\n",
      "auc.txt\t\t   guc.txt\t      miq.txt\t\t srp_latn.txt\r\n",
      "auv.txt\t\t   gug.txt\t      mkd.txt\t\t srr.txt\r\n",
      "ayr.txt\t\t   guj.txt\t      mlt.txt\t\t ssw.txt\r\n",
      "azj_cyrl.txt\t   guu.txt\t      mly_arab.txt\t suk.txt\r\n",
      "azj_latn.txt\t   gyr.txt\t      mly_latn.txt\t sun.txt\r\n",
      "bam.txt\t\t   hat_kreyol.txt     mos.txt\t\t sus.txt\r\n",
      "ban.txt\t\t   hat_popular.txt    mri.txt\t\t swe.txt\r\n",
      "bba.txt\t\t   hau_NE.txt\t      mxi.txt\t\t swh.txt\r\n",
      "bci.txt\t\t   hau_NG.txt\t      mxv.txt\t\t tah.txt\r\n",
      "bcl.txt\t\t   haw.txt\t      mya.txt\t\t taj.txt\r\n",
      "bel.txt\t\t   hea.txt\t      mzi.txt\t\t tam.txt\r\n",
      "bem.txt\t\t   heb.txt\t      nav.txt\t\t tat.txt\r\n",
      "ben.txt\t\t   hil.txt\t      nba.txt\t\t tbz.txt\r\n",
      "bfa.txt\t\t   hin.txt\t      nbl.txt\t\t tca.txt\r\n",
      "bho.txt\t\t   hlt.txt\t      ndo.txt\t\t tem.txt\r\n",
      "bin.txt\t\t   hms.txt\t      nds.txt\t\t tet.txt\r\n",
      "bis.txt\t\t   hna.txt\t      nep.txt\t\t tgk.txt\r\n",
      "blu.txt\t\t   hni.txt\t      nhn.txt\t\t tgl.txt\r\n",
      "boa.txt\t\t   hrv.txt\t      njo.txt\t\t tgl_tglg.txt\r\n",
      "bod.txt\t\t   hsb.txt\t      nku.txt\t\t tha.txt\r\n",
      "bos_cyrl.txt\t   hsf.txt\t      nld.txt\t\t tir.txt\r\n",
      "bos_latn.txt\t   hun.txt\t      nno.txt\t\t tiv.txt\r\n",
      "bre.txt\t\t   hus.txt\t      nob.txt\t\t tob.txt\r\n",
      "btb.txt\t\t   huu.txt\t      not.txt\t\t toi.txt\r\n",
      "bug.txt\t\t   hva.txt\t      nso.txt\t\t toj.txt\r\n",
      "bul.txt\t\t   hye.txt\t      nya_chechewa.txt\t ton.txt\r\n",
      "cab.txt\t\t   ibb.txt\t      nya_chinyanja.txt  top.txt\r\n",
      "cak.txt\t\t   ibo.txt\t      nym.txt\t\t tpi.txt\r\n",
      "cat.txt\t\t   ido.txt\t      nyn.txt\t\t tsn.txt\r\n",
      "cbi.txt\t\t   iii.txt\t      nzi.txt\t\t tso_MZ.txt\r\n",
      "cbr.txt\t\t   ike.txt\t      ojb.txt\t\t tsz.txt\r\n",
      "cbs.txt\t\t   ilo.txt\t      oss.txt\t\t tuk_cyrl.txt\r\n",
      "cbt.txt\t\t   ina.txt\t      ote.txt\t\t tuk_latn.txt\r\n",
      "cbu.txt\t\t   ind.txt\t      pam.txt\t\t tur.txt\r\n",
      "ccx.txt\t\t   isl.txt\t      pan.txt\t\t tyv.txt\r\n",
      "ceb.txt\t\t   ita.txt\t      pau.txt\t\t tzc.txt\r\n",
      "ces.txt\t\t   jav.txt\t      pbb.txt\t\t tzh.txt\r\n",
      "cha.txt\t\t   jiv.txt\t      pbu.txt\t\t tzm.txt\r\n",
      "chj.txt\t\t   jpn.txt\t      pcd.txt\t\t tzm_tfng.txt\r\n",
      "chk.txt\t\t   kal.txt\t      pcm.txt\t\t uig_arab.txt\r\n",
      "chr.txt\t\t   kan.txt\t      pes_1.txt\t\t uig_latn.txt\r\n",
      "cic.txt\t\t   kat.txt\t      pes_2.txt\t\t ukr.txt\r\n",
      "cjk.txt\t\t   kaz.txt\t      pis.txt\t\t umb.txt\r\n",
      "cjk_AO.txt\t   kbp.txt\t      plt.txt\t\t ura.txt\r\n",
      "cjs.txt\t\t   kde.txt\t      pnb.txt\t\t urd.txt\r\n",
      "ckb.txt\t\t   kea.txt\t      pol.txt\t\t uzn_cyrl.txt\r\n",
      "cmn_hans.txt\t   kek.txt\t      pon.txt\t\t uzn_latn.txt\r\n",
      "cmn_hant.txt\t   kha.txt\t      por_BR.txt\t vai.txt\r\n",
      "cnh.txt\t\t   khk.txt\t      por_PT.txt\t vec.txt\r\n",
      "cni.txt\t\t   khk_mong.txt       pov.txt\t\t ven.txt\r\n",
      "cof.txt\t\t   khm.txt\t      ppl.txt\t\t vep.txt\r\n",
      "cos.txt\t\t   kin.txt\t      prq.txt\t\t vie.txt\r\n",
      "cot.txt\t\t   kir.txt\t      prv.txt\t\t vmw.txt\r\n",
      "cpu.txt\t\t   kjh.txt\t      quc.txt\t\t war.txt\r\n",
      "crs.txt\t\t   kmb.txt\t      qud.txt\t\t wln.txt\r\n",
      "csa.txt\t\t   knc.txt\t      qug.txt\t\t wol.txt\r\n",
      "csw.txt\t\t   kng.txt\t      quy.txt\t\t wwa.txt\r\n",
      "ctd.txt\t\t   kng_AO.txt\t      quz.txt\t\t xho.txt\r\n",
      "cym.txt\t\t   koo.txt\t      qva.txt\t\t xsm.txt\r\n",
      "dag.txt\t\t   kor.txt\t      qvc.txt\t\t yad.txt\r\n",
      "dan.txt\t\t   kqn.txt\t      qvh.txt\t\t yao.txt\r\n",
      "ddn.txt\t\t   kri.txt\t      qvm.txt\t\t yap.txt\r\n",
      "deu.txt\t\t   krl.txt\t      qvn.txt\t\t ydd.txt\r\n",
      "deu_1901.txt\t   ktu.txt\t      qwh.txt\t\t ykg.txt\r\n",
      "dga.txt\t\t   kwi.txt\t      qxa.txt\t\t yor.txt\r\n",
      "dip.txt\t\t   lad.txt\t      qxn.txt\t\t yua.txt\r\n",
      "div.txt\t\t   lao.txt\t      qxu.txt\t\t zam.txt\r\n",
      "dyo.txt\t\t   lat.txt\t      rar.txt\t\t zro.txt\r\n",
      "dzo.txt\t\t   lat_1.txt\t      rmn.txt\t\t ztu.txt\r\n",
      "ell_monotonic.txt  lav.txt\t      rmn_1.txt\t\t zul.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls  /Users/siclemat/nltk_data/corpora/udhr2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['世界人权宣言', '联合国大会一九四八年十二月十日第217A', '(', 'III', ')', '号决议通过并颁布', '1948', '年', '12', '月', '10', '日', '，', '联合国大会通过并颁布', '《', '世界人权宣言', '》。', '这一具有历史意义的', '《', '宣言', '》', '颁布后', '，', '大会要求所有会员国广为宣传', '，', '并且', '“', '不分国家或领土的政治地位', ',', '主要在各级学校和其他教育机构加以传播', '、', '展示', '、', '阅读和阐述', '。”《', '宣言', '》', '全文如下', '：'], ['序言', '鉴于对人类家庭所有成员的固有尊严及其平等的和不移的权利的承认', ',', '乃是世界自由', '、', '正义与和平的基础', ',', '鉴于对人权的无视和侮蔑已发展为野蛮暴行', ',', '这些暴行玷污了人类的良心', ',', '而一个人人享有言论和信仰自由并免予恐惧和匮乏的世界的来临', ',', '已被宣布为普通人民的最高愿望', ',', '鉴于为使人类不致迫不得已铤而走险对暴政和压迫进行反叛', ',', '有必要使人权受法治的保护', ',', '鉴于有必要促进各国间友好关系的发展', ',', '鉴于各联合国国家的人民已在联合国宪章中重申他们对基本人权', '、', '人格尊严和价值以及男女平等权利的信念', ',', '并决心促成较大自由中的社会进步和生活水平的改善', ',', '鉴于各会员国业已誓愿同联合国合作以促进对人权和基本自由的普遍尊重和遵行', ',', '鉴于对这些权利和自由的普遍了解对于这个誓愿的充分实现具有很大的重要性', ',', '因此现在', ',', '大会', ',', '发布这一世界人权宣言', ',', '作为所有人民和所有国家努力实现的共同标准', ',', '以期每一个人和社会机构经常铭念本宣言', ',', '努力通过教诲和教育促进对权利和自由的尊重', ',', '并通过国家的和国际的渐进措施', ',', '使这些权利和自由在各会员国本身人民及在其管辖下领土的人民中得到普遍和有效的承认和遵行', ';'], ['第一条', '人人生而自由', ',', '在尊严和权利上一律平等', '。', '他们赋有理性和良心', ',', '并应以兄弟关系的精神相对待', '。']]\n"
     ]
    }
   ],
   "source": [
    "# http://www.iana.org/assignments/lang-tags/zh-cmn-Hans\n",
    "print(my_humanrights.sents('cmn_hans.txt')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type do the objects `gutenberg` and `my_humanrights` have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.plaintext.PlaintextCorpusReader'>\n",
      "<class 'nltk.corpus.reader.plaintext.PlaintextCorpusReader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(my_humanrights))\n",
    "print(type(gutenberg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "help(PlaintextCorpusReader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When reading a corpus directory, we can optionally specify  the sentence tokenizer (sentence splitter) and word tokenizer as well as the reader for paragraphs. This makes the reader class flexible and general!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading from URLs without NLTK\n",
    "How can we just download the text file from \"Deutsches Textarchiv\"?\n",
    "https://www.deutschestextarchiv.de/book/show/abschatz_gedichte_1704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = ('https ://www.deutschestextarchiv.de/book/' \n",
    "       'download_txt/abschatz_gedichte_1704')\n",
    "response = urllib.request.urlopen(url)\n",
    "data = response.read()      # a `bytes` object\n",
    "text = data.decode('utf-8') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:200])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "livereveal": {
   "center": false,
   "embedded": true,
   "enable_chalkboard": true,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "simple",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
