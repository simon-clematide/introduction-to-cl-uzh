{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLTK 1: Interactive exploration of corpora\n",
    "\n",
    "Learning goals:\n",
    "\n",
    "- How to install and import NLTK and its corpus data\n",
    "- How to use NLTK to explore text corpora interactively\n",
    "- Understand how useful raw text corpora can be\n",
    "- Understand what we can understand about language by quantitative and distributional corpus linguistic applications\n",
    "- Know how list comprehension helps to quickly do interactive exploration of corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "First, we need to install NLTK and download the book data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora/reuters.zip.\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/siclemat/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "Before we start working with NLTK, let's understand how Python imports work.\n",
    "\n",
    "### Statement: `import Module`\n",
    "\n",
    "When you import a module, you need to use fully qualified dot notation to access its objects and functions:\n",
    "\n",
    "```python\n",
    "# Import module book from package nltk\n",
    "import nltk.book\n",
    "\n",
    "# Objects and functions from nltk.book can only\n",
    "# be accessed with fully qualified dot notation.\n",
    "print(\"Second token from text1:\", nltk.book.text1[1])\n",
    "# Second token from text1: Moby\n",
    "\n",
    "# Objects and functions cannot be accessed directly:\n",
    "print(text1[1])  # This will raise a NameError\n",
    "# NameError: name 'text1' is not defined\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement: `from Module import *`\n",
    "\n",
    "Alternatively, you can import all objects and functions from a module directly into your namespace:\n",
    "\n",
    "```python\n",
    "# Load module book from package nltk and\n",
    "# import all its objects and functions into the current module\n",
    "from nltk.book import *\n",
    "\n",
    "# Objects and functions from nltk.book can be used without\n",
    "# dotted notation: package.module.object\n",
    "print(\"Second token of text1:\", text1[1])\n",
    "\n",
    "# The fully qualified dot notation does not work in this case\n",
    "print(\"Second token of text1:\", nltk.book.text1[1])  # This will raise a NameError\n",
    "# NameError: name 'nltk' is not defined\n",
    "```\n",
    "\n",
    "**Note:** Using `from Module import *` is convenient for interactive exploration but should be used carefully in production code to avoid namespace conflicts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading the NLTK Interactive Demo\n",
    "\n",
    "Note: This code is really meant for interactive exploration and prints out results more than returning values to compute with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Texts are sequences of tokens\n",
    "\n",
    "We can use the indexing or slicing notation to access tokens in a text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their type is `nltk.text.Text`, but their slices are simple lists of strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text1), type(text1[0]), type(text1[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create concordances: KWIC (Keyword in Context)\n",
    "\n",
    "Which text is \"Moby Dick\"? Which one is \"Sense and Sensibility\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"man\", lines=10, width=68)\n",
    "print()\n",
    "text2.concordance(\"man\", lines=10, width=68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"woman\", lines=10, width=68)\n",
    "print()\n",
    "text2.concordance(\"woman\", lines=10, width=68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word frequencies in a corpus\n",
    "\n",
    "Which book talks more about \"love\" independent of its length? Let's compute relative frequencies...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.count(\"love\") / len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.count(\"love\") / len(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ok, these numbers urgently need some formatting. Let's use format strings from Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Text1: {text1.count('love')/len(text1):.4%}\\nText2:\"\n",
    "    f\" {text2.count('love')/len(text2):.4%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequency distributions\n",
    "\n",
    "Calculate the frequency of all different tokens (=Types) in a text.\n",
    "Should follow the [Zipfian Law](https://en.wikipedia.org/wiki/Zipf%27s_law) for larger text corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(text1)\n",
    "vocabulary = sorted(fdist, key=fdist.get, reverse=True)\n",
    "for w in vocabulary[:20]:\n",
    "    print(w, \"\\t\\t\", fdist[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Printing a plot</h3>\n",
    "Make sure that the plot object is rendered by Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fdist.plot(20,cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a log-log plot to see if Zipf's law holds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the frequencies of words in the vocabulary, which is already sorted by frequency\n",
    "frequencies = [fdist[word] for word in vocabulary]\n",
    "# Generate the ranks for the words\n",
    "ranks = range(1, len(vocabulary) + 1)\n",
    "\n",
    "# Create the log-log plot\n",
    "plt.loglog(ranks, frequencies)\n",
    "plt.xlabel(\"Rank\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.title(\"Zipf's Law\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributional Similarity\n",
    "\n",
    "- \"You shall know a word by the company it keeps!\" (J. R. Firth, 1957)\n",
    "- \"words that occur in the same contexts tend to have similar meanings\" (Pantel, 2005)\n",
    "\n",
    "Which words do appear in similar contexts?\n",
    "\n",
    "### How does it work technically?\n",
    "\n",
    "- NLTK ranks similar words by the number of shared context pairs.\n",
    "- For each target word, NLTK collects all `(left-word, right-word)` contexts in which\n",
    "  it appears.\n",
    "- It then computes, for every other word, how many of these context pairs it shares.\n",
    "- The words are sorted in descending order of shared-context count.\n",
    "\n",
    "Thus the similar() output is a frequency-based ranking: words at the top occur in the largest number of identical left–right contexts as the target word.\n",
    "\n",
    "Again: Could you guess which text is \"Moby Dick\" and which one is \"Sense and\n",
    "Sensibility\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"woman\")\n",
    "print()\n",
    "text2.similar(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"love\")\n",
    "print()\n",
    "text2.similar(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical collocations\n",
    "\n",
    "Which word pairs occur more often than expected by chance?\n",
    "\n",
    "-     **Expected frequency**: Assume each word occurs independently according to its unigram frequency. Drawing two words consecutively from an urn models the probability of a bigram occurring by chance.\n",
    "-     **Empirical frequency**: Compute the actual bigram distribution observed in the corpus.\n",
    "-     A word pair is a [**statistical collocation**] (https://en.wikipedia.org/wiki/Collocation) when its observed bigram frequency substantially exceeds the expected frequency, indicating a non-random association.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text1.collocation_list())\n",
    "print()\n",
    "print(text2.collocation_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dispersion plots\n",
    "\n",
    "How are specific words distributed across a chronological sequence of texts?  \n",
    "Example: _U.S. Inaugural Addresses_\n",
    "\n",
    "- The timeline is represented implicitly by the **ordered sequence of speeches**.\n",
    "- A dispersion plot marks each occurrence of a word along this sequence, showing **when** and **how frequently** it appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"freedom\", \"war\"])\n",
    "text4.dispersion_plot([\"economy\", \"war\", \"digital\", \"slavery\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequency-Based Text Generation\n",
    "\n",
    "A simple n-gram language model predicts the next word from the **preceding n−1 words**.\n",
    "\n",
    "- Build an **n-gram frequency table** from the corpus (e.g. trigrams).\n",
    "- Convert counts into **conditional probabilities**:  \n",
    "  P(next_word | w1, w2) = count(w1, w2, next_word) / count(w1, w2)\n",
    "- Generate text by repeatedly **sampling the next word** from this conditional distribution.\n",
    "- Resulting output reflects local phrase patterns learned from the presidential speeches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = text4.generate(text_seed=\"Freedom\".split(), length=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 2025's generative AI language models, we know what is possible. But here are some\n",
    "earlier approaches that represent important steps in the development of text generation\n",
    "(and can be even more fun to play with):\n",
    "\n",
    "Text generation using recursive neural networks from 2015, which can take a little more of the already expressed material into account when proposing the next word: https://cyborg.tenso.rs\n",
    "\n",
    "- Recommended: Language model of (re-)tweets by/with Donald Trump (e.g. start with \"America\")\n",
    "- Start with \"I love\" and select different training corpora (e.g. Linux:-)\n",
    "\n",
    "Early GPT-2 transformer-based text generation:\n",
    "\n",
    "- Write your next ACL paper with it: [This paper describes](https://transformer.huggingface.co/doc/arxiv-nlp/BcKBkznNiWnDfJdynrvMxQkF/edit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing XML-based Corpora\n",
    "\n",
    "Theater plays have a more complex structure than raw texts.\n",
    "NLTK contains XML-encoded Shakespeare. See this excerpt:\n",
    "\n",
    "```\n",
    "<PLAY>\n",
    "  <TITLE>The Tragedy of Othello</TITLE>\n",
    "  <PERSONAE>\n",
    "    <PERSONA>DUKE OF VENICE</PERSONA>\n",
    "    ...\n",
    "  </PERSONAE>\n",
    "\n",
    "  <ACT>\n",
    "    <SCENE>\n",
    "      <SPEECH>\n",
    "        <SPEAKER>RODERIGO</SPEAKER>\n",
    "        <LINE>Tush! never tell me; I take it much unkindly</LINE>\n",
    "      </SPEECH>\n",
    "    </SCENE>\n",
    "  </ACT>\n",
    "</PLAY>\n",
    "```\n",
    "\n",
    "Let's use the NLTK XML reader to load the NLTK book samples of Shakespeare plays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import shakespeare\n",
    "\n",
    "# Read XML tree for one play\n",
    "tree = shakespeare.xml(\"othello.xml\")\n",
    "\n",
    "# Extract all speaker–line pairs (first scene)\n",
    "scene = tree.find(\".//SCENE\")\n",
    "for speech in scene.findall(\".//SPEECH\"):\n",
    "    speaker = speech.findtext(\"SPEAKER\")\n",
    "    lines = [l.text for l in speech.findall(\"LINE\")]\n",
    "    print(speaker, lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
