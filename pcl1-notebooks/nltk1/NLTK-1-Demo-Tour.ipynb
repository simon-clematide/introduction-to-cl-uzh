{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLTK 1: Interactive exploration of corpora\n",
    "\n",
    "Learning goals:\n",
    "\n",
    "- How to install and import NLTK and its corpus data\n",
    "- How to use NLTK to explore text corpora interactively\n",
    "- Understand how useful raw text corpora can be\n",
    "- Understand what we can understand about language by quantitative and distributional corpus linguistic applications\n",
    "- Know how list comprehension helps to quickly do interactive exploration of corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "First, we need to install NLTK and download the book data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "Before we start working with NLTK, let's understand how Python imports work.\n",
    "\n",
    "### Statement: `import Module`\n",
    "\n",
    "When you import a module, you need to use fully qualified dot notation to access its objects and functions:\n",
    "\n",
    "```python\n",
    "# Import module book from package nltk\n",
    "import nltk.book\n",
    "\n",
    "# Objects and functions from nltk.book can only\n",
    "# be accessed with fully qualified dot notation.\n",
    "print(\"Second token from text1:\", nltk.book.text1[1])\n",
    "# Second token from text1: Moby\n",
    "\n",
    "# Objects and functions cannot be accessed directly:\n",
    "print(text1[1])  # This will raise a NameError\n",
    "# NameError: name 'text1' is not defined\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement: `from Module import *`\n",
    "\n",
    "Alternatively, you can import all objects and functions from a module directly into your namespace:\n",
    "\n",
    "```python\n",
    "# Load module book from package nltk and\n",
    "# import all its objects and functions into the current module\n",
    "from nltk.book import *\n",
    "\n",
    "# Objects and functions from nltk.book can be used without\n",
    "# dotted notation: package.module.object\n",
    "print(\"Second token of text1:\", text1[1])\n",
    "\n",
    "# The fully qualified dot notation does not work in this case\n",
    "print(\"Second token of text1:\", nltk.book.text1[1])  # This will raise a NameError\n",
    "# NameError: name 'nltk' is not defined\n",
    "```\n",
    "\n",
    "**Note:** Using `from Module import *` is convenient for interactive exploration but should be used carefully in production code to avoid namespace conflicts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading the NLTK Interactive Demo\n",
    "\n",
    "Note: This code is really meant for interactive exploration and prints out results more than returning values to compute with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Texts are sequences of tokens\n",
    "\n",
    "We can use the indexing or slicing notation to access tokens in a text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their type is `nltk.text.Text`, but their slices are simple lists of strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text1), type(text1[0]), type(text1[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create concordances: KWIC (Keyword in Context)\n",
    "\n",
    "Which text is \"Moby Dick\"? Which one is \"Sense and Sensibility\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"man\", lines=10, width=68)\n",
    "print()\n",
    "text2.concordance(\"man\", lines=10, width=68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"woman\", lines=10, width=68)\n",
    "print()\n",
    "text2.concordance(\"woman\", lines=10, width=68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word frequencies in a corpus\n",
    "\n",
    "Which book talks more about \"love\" independent of its length? Let's compute relative frequencies...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.count(\"love\") / len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.count(\"love\") / len(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ok, these numbers urgently need some formatting. Let's use format strings from Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Text1: {text1.count('love')/len(text1):.4%}\\nText2:\"\n",
    "    f\" {text2.count('love')/len(text2):.4%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequency distributions\n",
    "\n",
    "Calculate the frequency of all different tokens (=Types) in a text.\n",
    "Should follow the [Zipfian Law](https://en.wikipedia.org/wiki/Zipf%27s_law) for larger text corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(text1)\n",
    "vocabulary = sorted(fdist, key=fdist.get, reverse=True)\n",
    "for w in vocabulary[:20]:\n",
    "    print(w, \"\\t\\t\", fdist[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Printing a plot</h3>\n",
    "Make sure that the plot object is rendered by Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fdist.plot(20,cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a log-log plot to see if Zipf's law holds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the frequencies of words in the vocabulary, which is already sorted by frequency\n",
    "frequencies = [fdist[word] for word in vocabulary]\n",
    "# Generate the ranks for the words\n",
    "ranks = range(1, len(vocabulary) + 1)\n",
    "\n",
    "# Create the log-log plot\n",
    "plt.loglog(ranks, frequencies)\n",
    "plt.xlabel(\"Rank\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.title(\"Zipf's Law\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributional Similarity\n",
    "\n",
    "- \"You shall know a word by the company it keeps!\" (J. R. Firth, 1957)\n",
    "- \"words that occur in the same contexts tend to have similar meanings\" (Pantel, 2005)\n",
    "\n",
    "Which words do appear in similar contexts?\n",
    "\n",
    "### How does it work technically?\n",
    "\n",
    "- NLTK ranks similar words by the number of shared context pairs.\n",
    "- For each target word, NLTK collects all `(left-word, right-word)` contexts in which\n",
    "  it appears.\n",
    "- It then computes, for every other word, how many of these context pairs it shares.\n",
    "- The words are sorted in descending order of shared-context count.\n",
    "\n",
    "Thus the similar() output is a frequency-based ranking: words at the top occur in the largest number of identical left–right contexts as the target word.\n",
    "\n",
    "Again: Could you guess which text is \"Moby Dick\" and which one is \"Sense and\n",
    "Sensibility\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"woman\")\n",
    "print()\n",
    "text2.similar(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"love\")\n",
    "print()\n",
    "text2.similar(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical collocations\n",
    "\n",
    "Which word pairs occur more often than expected by chance?\n",
    "\n",
    "-     **Expected frequency**: Assume each word occurs independently according to its unigram frequency. Drawing two words consecutively from an urn models the probability of a bigram occurring by chance.\n",
    "-     **Empirical frequency**: Compute the actual bigram distribution observed in the corpus.\n",
    "-     A word pair is a [**statistical collocation**] (https://en.wikipedia.org/wiki/Collocation) when its observed bigram frequency substantially exceeds the expected frequency, indicating a non-random association.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text1.collocation_list())\n",
    "print()\n",
    "print(text2.collocation_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dispersion plots\n",
    "\n",
    "How are specific words distributed across a chronological sequence of texts?  \n",
    "Example: _U.S. Inaugural Addresses_\n",
    "\n",
    "- The timeline is represented implicitly by the **ordered sequence of speeches**.\n",
    "- A dispersion plot marks each occurrence of a word along this sequence, showing **when** and **how frequently** it appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"freedom\", \"war\"])\n",
    "text4.dispersion_plot([\"economy\", \"war\", \"digital\", \"slavery\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequency-Based Text Generation\n",
    "\n",
    "A simple n-gram language model predicts the next word from the **preceding n−1 words**.\n",
    "\n",
    "- Build an **n-gram frequency table** from the corpus (e.g. trigrams).\n",
    "- Convert counts into **conditional probabilities**:  \n",
    "  P(next_word | w1, w2) = count(w1, w2, next_word) / count(w1, w2)\n",
    "- Generate text by repeatedly **sampling the next word** from this conditional distribution.\n",
    "- Resulting output reflects local phrase patterns learned from the presidential speeches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = text4.generate(text_seed=\"Freedom\".split(), length=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 2025's generative AI language models, we know what is possible. But here are some\n",
    "earlier approaches that represent important steps in the development of text generation\n",
    "(and can be even more fun to play with):\n",
    "\n",
    "Text generation using recursive neural networks from 2015, which can take a little more of the already expressed material into account when proposing the next word: https://cyborg.tenso.rs\n",
    "\n",
    "- Recommended: Language model of (re-)tweets by/with Donald Trump (e.g. start with \"America\")\n",
    "- Start with \"I love\" and select different training corpora (e.g. Linux:-)\n",
    "\n",
    "Early GPT-2 transformer-based text generation:\n",
    "\n",
    "- Write your next ACL paper with it: [This paper describes](https://transformer.huggingface.co/doc/arxiv-nlp/BcKBkznNiWnDfJdynrvMxQkF/edit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing XML-based Corpora\n",
    "\n",
    "Theater plays have a more complex structure than raw texts.\n",
    "NLTK contains XML-encoded Shakespeare. See this excerpt:\n",
    "\n",
    "```\n",
    "<PLAY>\n",
    "  <TITLE>The Tragedy of Othello</TITLE>\n",
    "  <PERSONAE>\n",
    "    <PERSONA>DUKE OF VENICE</PERSONA>\n",
    "    ...\n",
    "  </PERSONAE>\n",
    "\n",
    "  <ACT>\n",
    "    <SCENE>\n",
    "      <SPEECH>\n",
    "        <SPEAKER>RODERIGO</SPEAKER>\n",
    "        <LINE>Tush! never tell me; I take it much unkindly</LINE>\n",
    "      </SPEECH>\n",
    "    </SCENE>\n",
    "  </ACT>\n",
    "</PLAY>\n",
    "```\n",
    "\n",
    "Let's use the NLTK XML reader to load the NLTK book samples of Shakespeare plays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import shakespeare\n",
    "\n",
    "# Read XML tree for one play\n",
    "tree = shakespeare.xml(\"othello.xml\")\n",
    "\n",
    "# Extract all speaker–line pairs (first scene)\n",
    "scene = tree.find(\".//SCENE\")\n",
    "for speech in scene.findall(\".//SPEECH\"):\n",
    "    speaker = speech.findtext(\"SPEAKER\")\n",
    "    lines = [l.text for l in speech.findall(\"LINE\")]\n",
    "    print(speaker, lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
